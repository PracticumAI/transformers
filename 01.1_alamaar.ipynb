{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "# *Practicum AI:* Transformers - Alamaar Transformer\n",
    "\n",
    "This exercise adapted from Alamaar (2020) <a href=\"https://jalammar.github.io/illustrated-transformer/\"> The Illustrated Transformer</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Resources\n",
    "- [Jay Alamaar (Transformers)](https://www.youtube.com/watch?v=-QH8fRhqFHM&t=2s) - A complete description of this exercise begins at the 15:00 minute mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "SfaGYwr6QGl0",
    "outputId": "45caa369-4bda-420d-f407-1002204c6e83"
   },
   "outputs": [],
   "source": [
    "# If you are running on Google Colab or outside of HiPerGator\n",
    "# uncomment the following line to install the needed packages\n",
    "# HiPerGator users should not need to do this!\n",
    "#\n",
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Tokenization\n",
    "\n",
    "Declare and assign values to the tokenizer and model variables.  Distilgpt2 is a smaller version of the GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKOHXyzNQ5Wa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:06:49.786124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\") \n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a value to the text string to be tokenized, and then present it to the model's generate function.  The model correctly returns 'Redemption' as the next word in the sequence.\n",
    "\n",
    "```python\n",
    "text = \"The Shawshank\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(input, max_length = 5, do_sample = False)\n",
    "\n",
    "# Print the output\n",
    "print('\\n',tokenizer.decode(output[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zdQBaZZcQ_Sa",
    "outputId": "4cc3dbf5-922d-4eda-b47e-75e8adbc96e2"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Print the token ides (of the input and output)\n",
    "output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m8m1rK3gNNW4",
    "outputId": "61da2785-cd28-4b00-84bf-ac65deea8d57"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tProbSeTJATA"
   },
   "source": [
    "### From words to vectors and back\n",
    "\n",
    "```python\n",
    "# Print the input token ids\n",
    "text = \"The Shawshank\"\n",
    "input = tokenizer(text, return_tensors=\"pt\")['input_ids']\n",
    "input\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Dy2Pjd--ROa5",
    "outputId": "1227d405-a6b1-48bd-f6e6-127ea739cc04"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tokenizer.convert_ids_to_tokens(input[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sTGffdCOJbdo",
    "outputId": "e45abbde-d575-40bb-82b8-48dc2c90ffb3"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lbn94y0P2UV"
   },
   "source": [
    "### Breathe meaning into numbers (Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a vocabulary of 50,257 tokens, each with an embedding of 768 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5QCFBcxZQIN8",
    "outputId": "78f4012c-3c7c-4c89-813a-62abae5cd98d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the embedding matrix of the model\n",
    "model.transformer.wte # Dimensions are: (Number of tokens in vocabulary, dimension of model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# View all of the embeddings.\n",
    "model.transformer.wte.weight\n",
    "\n",
    "# View the embedding vector for token #464 ('The')\n",
    "model.transformer.wte.weight[464]\n",
    "\n",
    "# View the size of the embedding vector for token #464 \n",
    "len(model.transformer.wte.weight[464])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ah9tc1gP7lX"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the model's generative abilities.\n",
    "\n",
    "```python\n",
    "\n",
    "text = \"The chicken didn't cross the road because it was\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(input, max_length = 25, do_sample = True)\n",
    "\n",
    "# Print the output\n",
    "print('\\n',tokenizer.decode(output[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZT5lmGVK60mJ",
    "outputId": "3c82374f-eee5-4e19-ef64-e0908b5719ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The chicken didn't cross the road because it was like, \"Oh wow. That's the best\n"
     ]
    }
   ],
   "source": [
    "# Code it!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOEWirarSiFWRClhLdkaXby",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Simple Transformer Language Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow-2.4.1",
   "language": "python",
   "name": "tensorflow-2.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
